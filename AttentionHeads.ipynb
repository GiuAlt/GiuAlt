{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFlq0PLp4Lus74EqKFDk9A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiuAlt/GiuAlt/blob/main/AttentionHeads.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Ya_DYuEFbQjG"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(Q,K,V):\n",
        "\n",
        " \"\"\"\n",
        " args:\n",
        "  Q: (B,n,d_k)\n",
        "  K: (B,n,d_k) ## these need to be transposed to do the dot produce\n",
        "  V: (B,n,d_v)\n",
        " \"\"\"\n",
        " raw_attention_scores = Q @ K.transpose (-2,-1) # (B,n,n)\n",
        " d_k = K.shape[-1]\n",
        " scaled_attention_scores = raw_attention_scores/0.5**d_k\n",
        " #attention_weights = torch.softmax(scaled_attention_scores)\n",
        " attention_weights = rorch.exp(scaled_attention_scores)/torch.exp(scaled_attention_scores).sum(dim = -1, keep_dimensions = True)\n",
        " return attention_weights @ V"
      ],
      "metadata": {
        "id": "Xa1nHSCKbRnx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def causal_self_attention(Q,K,V):\n",
        "## make a mask\n",
        " \"\"\"\n",
        " args:\n",
        "  Q: (B,n,d_k)\n",
        "  K: (B,n,d_k) ## these need to be transposed to do the dot produce\n",
        "  V: (B,n,d_v)\n",
        " \"\"\"\n",
        " raw_attention_scores = Q @ K.transpose (-2,-1) # (B,n,n)\n",
        " d_k = K.shape[-1]\n",
        " scaled_attention_scores = raw_attention_scores/0.5**d_k\n",
        " #attention_weights = torch.softmax(scaled_attention_scores)\n",
        " mask = torch.triu(torch.ones(n,n, device = Q.device, diagonal = 1))## torch triu with Diagonal =1 makes all values above the diagonal a certain value.\n",
        " masked_attention_scores = scaled_attention_scores + mask\n",
        " attention_weights = torch.exp(masked_attention_scores)/torch.exp(scaled_attention_scores).sum(dim = -1, keep_dimensions = True)\n",
        " return attention_weights @ V"
      ],
      "metadata": {
        "id": "q5MmR8KTcONi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "B = 2\n",
        "n = 10\n",
        "d_k = 16\n",
        "d_v = 16 ## this should determine the shape, it should be the same as z in the vector torch.size([x,y,z])\n",
        "Q = torch.randn(B,n,d_k)\n",
        "K = torch.randn(B,n,d_k)\n",
        "v = torch.randn(B,n,d_v)"
      ],
      "metadata": {
        "id": "feNPbzD5fGWJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02aqDgtCfR4M",
        "outputId": "252521f3-9518-44d6-f6ef-383656718097"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 10, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print()"
      ],
      "metadata": {
        "id": "nefn9ADnfUNC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}